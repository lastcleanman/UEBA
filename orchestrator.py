import sys
import traceback
import os
import json
import importlib
import urllib.request
import glob
import time  # ì‹¤ì‹œê°„ ë°˜ë³µì„ ìœ„í•´ ì¶”ê°€
from datetime import datetime
import pandas as pd
if "/UEBA" not in sys.path:
    sys.path.insert(0, "/UEBA")

from common.setup.spark_manager import get_spark_session
from common.setup.logger import get_logger
from common.setup.config import ES_HOST, ES_PORT, ES_INDEX_NAME, CONFIG_DIR
from common.ingestion.data_reader import fetch_data
from common.processing.normalizer import normalize_data
from pyspark.sql.functions import col, lit

logger = get_logger("Orchestrator")

WATERMARK_FILE = "/UEBA/watermark.json"

def get_last_ts(source_name):
    """ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘í•œ ê¸°ì¤€ ì‹œê°„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        if os.path.exists(WATERMARK_FILE):
            with open(WATERMARK_FILE, "r") as f:
                data = json.load(f)
                return data.get(source_name, "1970-01-01 00:00:00")
    except Exception as e:
        logger.error(f"Watermark ì½ê¸° ì‹¤íŒ¨: {e}")
    return "1970-01-01 00:00:00"

def set_last_ts(source_name, ts):
    """ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„° ì¤‘ ê°€ì¥ ìµœì‹  ì‹œê°„ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        data = {}
        if os.path.exists(WATERMARK_FILE):
            with open(WATERMARK_FILE, "r") as f:
                data = json.load(f)
        data[source_name] = str(ts)
        with open(WATERMARK_FILE, "w") as f:
            json.dump(data, f)
    except Exception as e:
        logger.error(f"Watermark ì €ì¥ ì‹¤íŒ¨: {e}")

def save_history(source, index, count, status, error=""):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ì‹œ MariaDBì— ì´ë ¥ì„ ë‚¨ê¸°ëŠ” í•¨ìˆ˜ (ìµœì í™” ë²„ì „)"""
    try:
        from sqlalchemy import create_engine, text
        # DB ì—°ê²° ì„¤ì •
        db_url = "mysql+pymysql://ueba_user:Suju!0901@192.168.0.131:13306/UEBA_TEST"
        engine = create_engine(db_url, pool_pre_ping=True)
        
        with engine.begin() as conn:
            # 1. í…Œì´ë¸” ìƒì„± (ì—†ì„ ê²½ìš°ì—ë§Œ)
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS ueba_ingestion_history (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    collect_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                    source_name VARCHAR(50),
                    target_index VARCHAR(50),
                    count INT,
                    status VARCHAR(20),
                    error_message TEXT
                )
            """))
            
            # 2. ì´ë ¥ ì‚½ì… (Named Parameter ë°©ì‹ ì‚¬ìš©)
            conn.execute(text("""
                INSERT INTO ueba_ingestion_history (source_name, target_index, count, status, error_message)
                VALUES (:source, :index, :count, :status, :error)
            """), {
                "source": source, 
                "index": index, 
                "count": count, 
                "status": status, 
                "error": error
            })
            
        logger.info(f"ğŸ“œ [History] {source} -> {count}ê±´ ê¸°ë¡ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ DB ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")

def reset_and_init_es():
    """Elasticsearch ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì²˜ìŒ ê°€ë™ ì‹œ 1íšŒ ê¶Œì¥)"""
    es_url = f"http://{ES_HOST}:{ES_PORT}/{ES_INDEX_NAME}"
    try:
        req_del = urllib.request.Request(es_url, method="DELETE")
        urllib.request.urlopen(req_del)
        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ: {ES_INDEX_NAME}")
    except: pass
    
    mapping = { "mappings": { "properties": {
        "final_ts": { 
            "type": "date", 
            # yyyy-MM-dd+HH:mm í˜•íƒœë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì›í•˜ë„ë¡ ìˆ˜ì •
            "format": "yyyy-MM-dd+HH:mm||yyyy-MM-dd||yyyy-MM-dd'T'HH:mm:ss||yyyy-MM-dd HH:mm:ss||yyyy-MM-ddZ||strict_date_optional_time||epoch_millis" 
        },
        "log_source": { "type": "keyword" },
        "user_id": { "type": "keyword" },
        "action": { "type": "keyword" },
        "risk_score": { "type": "double" }
    }}}
    
    try:
        req = urllib.request.Request(es_url, data=json.dumps(mapping).encode("utf-8"), method="PUT")
        req.add_header("Content-Type", "application/json")
        urllib.request.urlopen(req)
        logger.info(f"âœ… ES ë§¤í•‘ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ES ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def run_pipeline(spark, active_plugins):
    """ì‹¤ì œ ìˆ˜ì§‘ ë° ë¶„ì„ í”„ë¡œì„¸ìŠ¤ (ë°˜ë³µ ì‹¤í–‰ë¨)"""
    # 1. ì„¤ì •ëœ ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ ì½ê¸° (*_sources.json)
    sources = []
    source_files = glob.glob(f"{CONFIG_DIR}/*_sources.json")
    
    for file_path in source_files:
        with open(file_path, "r", encoding="utf-8") as f:
            file_sources = json.load(f)
            sources.extend([s for s in file_sources if s.get("enabled", True)])

    total_processed = 0
    for source in sources:
        # ueba-webserverì—ì„œ ë„˜ì–´ì˜¨ ë‚ ì§œë³„ ì™€ì¼ë“œì¹´ë“œ ê²½ë¡œ ì²˜ë¦¬ í¬í•¨
        raw_pandas_df = fetch_data(source)
        
        if raw_pandas_df is None or raw_pandas_df.empty:
            continue
        
        source_name = source.get('name', 'Unknown')
        
        try:
            # â­ï¸ [í•µì‹¬ ìˆ˜ì •]: Pandas DFë¥¼ PySparkê°€ ì‹«ì–´í•˜ëŠ” Arrow ë°©ì‹ ì—†ì´ ì•ˆì „í•˜ê²Œ Spark DFë¡œ ë³€í™˜
            # 1. NaN, NaT ê°’ì„ Noneìœ¼ë¡œ ì¹˜í™˜ (ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€)
            safe_pandas_df = raw_pandas_df.replace({pd.NA: None}).where(pd.notnull(raw_pandas_df), None)
            
            # 2. DataFrameì„ Dictionary List í˜•ì‹ìœ¼ë¡œ í’€ì–´ë²„ë¦¼
            dict_list = safe_pandas_df.to_dict(orient='records')
            
            # 3. í’€ì–´ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¹¨ë—í•œ Spark DataFrame ìƒì„±
            spark_df = spark.createDataFrame(dict_list)
            
        except Exception as e:
            logger.error(f"âŒ [{source_name}] PySpark ë³€í™˜ ì‹¤íŒ¨. ì›ë³¸ ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”: {e}")
            continue

        # ì •ê·œí™” ë° í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰
        clean_df = normalize_data(spark, spark_df, source_name)
        
        # â­ï¸ [ì¦ë¶„ ìˆ˜ì§‘ ë¡œì§: ê°•ë ¥ ë²„ì „] â­ï¸
        last_ts = get_last_ts(source_name)
        logger.info(f"ğŸ” [{source_name}] ì €ì¥ëœ Watermark: {last_ts}")
        
        # Spark ì „ìš© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í™•ì‹¤í•˜ê²Œ ë¹„êµ (ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜ í›„ í¬ê¸° ë¹„êµ)
        clean_df = clean_df.filter(col("final_ts").cast("string") > lit(str(last_ts)))
        
        current_count = clean_df.count()
        
        if current_count == 0:
            logger.info(f"â© [{source_name}] ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        # ìƒˆ ë°ì´í„° ì¤‘ ê°€ì¥ ìµœì‹  ì‹œê°„ ì°¾ì•„ì„œ ì €ì¥
        max_row = clean_df.agg({"final_ts": "max"}).collect()[0]
        max_ts = max_row[0] if max_row else None
        
        if max_ts:
            set_last_ts(source_name, max_ts)
            logger.info(f"ğŸ’¾ [{source_name}] Watermark ê°±ì‹  ì™„ë£Œ: {max_ts}")
        # â­ï¸ [ì¦ë¶„ ìˆ˜ì§‘ ë¡œì§ ë] â­ï¸

        # í”ŒëŸ¬ê·¸ì¸ ì²˜ë¦¬ ë° ì´ë ¥ ì €ì¥
        detected_df = load_and_run_plugins(clean_df, active_plugins.get("detection", []), "Detection")
        load_and_run_plugins(detected_df, active_plugins.get("loading", []), "Loading")
        load_and_run_plugins(detected_df, active_plugins.get("notification", []), "Notification")
        
        save_history(source_name, ES_INDEX_NAME, current_count, "SUCCESS")
        total_processed += current_count

    if total_processed > 0:
        logger.info(f"--- ì²˜ë¦¬ ì™„ë£Œ ({total_processed}ê±´) / DB ì´ë ¥ ê¸°ë¡ ì•ˆí•¨(í…ŒìŠ¤íŠ¸ìš©) ---")
        
    return total_processed

def load_and_run_plugins(df, plugin_list, step_name):
    for plugin_path in plugin_list:
        try:
            plugin_module = importlib.import_module(plugin_path)
            if hasattr(plugin_module, "execute"):
                df = plugin_module.execute(df)
        except Exception as e:
            logger.error(f"âŒ {step_name} í”ŒëŸ¬ê·¸ì¸ {plugin_path} ì‹¤íŒ¨: {e}")
    return df

def main():
    logger.info("ğŸš€ UEBA ì‹¤ì‹œê°„ ì›ê²© ìˆ˜ì§‘ ì—”ì§„ ì‹œì‘")
    
    # ì²˜ìŒ ì‹¤í–‰ ì‹œ í•œ ë²ˆ ì¸ë±ìŠ¤ ì •ë¦¬ (í•„ìš”ì— ë”°ë¼ ì£¼ì„ ì²˜ë¦¬)
    reset_and_init_es()
    
    spark = get_spark_session()
    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
    spark.conf.set("spark.sql.execution.arrow.pyspark.fallback.enabled", "true")
    config_path = "/UEBA/pipeline_config.json"
    with open(config_path, "r") as f:
        config = json.load(f)
    active_plugins = config.get("active_plugins", {})

    try:
        while True:
            logger.info(f"\n--- {datetime.now()} ìˆ˜ì§‘ ì£¼ê¸° ì‹œì‘ ---")
            count = run_pipeline(spark, active_plugins)
            logger.info(f"--- ì²˜ë¦¬ ì™„ë£Œ ({count}ê±´) / 30ì´ˆ ëŒ€ê¸° ---")
            time.sleep(30) # 10ì´ˆë§ˆë‹¤ ìƒˆ ë¡œê·¸ ì²´í¬
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ìˆ˜ì§‘ ì—”ì§„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()