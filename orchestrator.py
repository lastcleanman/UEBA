import sys
import traceback
import os
import json
import importlib
import urllib.request
import glob
import time
from datetime import datetime
import pandas as pd
import xml.etree.ElementTree as ET
from xml.dom import minidom
from sqlalchemy import create_engine, text

if "/UEBA" not in sys.path:
    sys.path.insert(0, "/UEBA")

from common.setup.spark_manager import get_spark_session
from common.setup.logger import get_logger
from common.setup.config import ES_HOST, ES_PORT, ES_INDEX_NAME, CONFIG_DIR
from common.ingestion.data_reader import fetch_data
from common.processing.normalizer import normalize_data
from pyspark.sql.functions import col, lit

logger = get_logger("Orchestrator")

WATERMARK_FILE = "/UEBA/watermark.json"
PARSER_DIR = "/UEBA/common/parser"
DB_SOURCES_PATH = "/UEBA/common/setup/db_sources.json"

# --- [DB ì„¤ì • ë° ì—”ì§„ ìƒì„±] ---
def get_db_engine_by_name(db_name="ueba_mariaDB"):
    try:
        if not os.path.exists(DB_SOURCES_PATH):
            logger.error(f"âŒ DB ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DB_SOURCES_PATH}")
            return None
            
        with open(DB_SOURCES_PATH, "r", encoding="utf-8") as f:
            data_sources = json.load(f)
            
        if isinstance(data_sources, list):
            conf = next((s for s in data_sources if s.get("name") == db_name), None)
        else:
            conf = data_sources if data_sources.get("name") == db_name else None

        if not conf:
            logger.error(f"âŒ '{db_name}' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        target_db = conf.get('database')
        db_url = f"mysql+pymysql://{conf['user']}:{conf['password']}@{conf['host']}:{conf['port']}/{target_db}"
        return create_engine(db_url, pool_pre_ping=True)
        
    except Exception as e:
        logger.error(f"âŒ DB ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

db_engine = get_db_engine_by_name("ueba_mariaDB")

# --- [Step 1~3] ììœ¨ í•™ìŠµ ë° íŒŒì„œ ì €ì¥ ---
def auto_learn_and_save_parsers():
    if db_engine is None: return
    
    logger.info("ğŸ•µï¸ [Step 1-3] ì‹ ê·œ íŒ¨í„´ í•™ìŠµ ë° íŒŒì„œ ì—…ë°ì´íŠ¸ ì‹œì‘")
    if not os.path.exists(PARSER_DIR): os.makedirs(PARSER_DIR, exist_ok=True)

    log_files = glob.glob("/UEBA/data/logs/*.log")
    inference_map = {
        "user": "user_id", "user_id": "user_id",
        "ip": "src_ip", "src_ip": "src_ip",
        "department": "department", "action": "action",
        "device_id": "device_id", "resource": "resource"
    }

    for file_path in log_files:
        filename = os.path.basename(file_path)
        source_name = "Unknown"
        if "authentication" in filename: source_name = "Auth_Logs"
        elif "webserver" in filename: source_name = "Web_Logs"
        elif "endpoint" in filename: source_name = "Endpoint_Logs"
        elif "firewall" in filename: source_name = "Firewall_Logs"
        
        if source_name == "Unknown": continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                line = f.readline().strip()
                if not line: continue
                sample = json.loads(line)
            
            root = ET.Element("parser", name=source_name)
            has_user = False
            for k, v in sample.items():
                if k in inference_map:
                    target = inference_map[k]
                    ET.SubElement(root, "field", target=target, source=k)
                    if target == "user_id": has_user = True
            
            if has_user:
                ET.SubElement(root, "field", target="emp_name", source="mapped_name")

            xml_str = minidom.parseString(ET.tostring(root)).toprettyxml(indent="    ")

            with open(os.path.join(PARSER_DIR, f"{source_name}.xml"), "w", encoding="utf-8") as xf:
                xf.write(xml_str)
            
            with db_engine.begin() as conn:
                conn.execute(text("""
                    INSERT INTO sj_ueba_parsers (source_name, parser_xml)
                    VALUES (:source, :xml)
                    ON DUPLICATE KEY UPDATE parser_xml = :xml, updated_at = CURRENT_TIMESTAMP
                """), {"source": source_name, "xml": xml_str})
            logger.info(f"âœ… [{source_name}] íŒŒì„œ ë™ê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ [{source_name}] í•™ìŠµ ì‹¤íŒ¨: {e}")

# --- [Step 4] ìˆ˜ì§‘ ì´ë ¥ ì €ì¥ í•¨ìˆ˜ ---
def save_history(source, count, status, error="", start_time=None):
    if db_engine is None: return
    try:
        with db_engine.begin() as conn:
            conn.execute(text("""
                INSERT INTO sj_ueba_ingestion_history (source_name, processed_count, status, error_message, start_time)
                VALUES (:source, :count, :status, :error, :start)
            """), {
                "source": source, "count": count, "status": status, "error": error, "start": start_time
            })
        logger.info(f"ğŸ“œ [History] {source} ì²˜ë¦¬ ì´ë ¥ ê¸°ë¡ ì™„ë£Œ (ê±´ìˆ˜: {count})")
    except Exception as e:
        logger.warning(f"âš ï¸ DB ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")

# --- [ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def get_last_ts(source_name):
    try:
        if os.path.exists(WATERMARK_FILE):
            with open(WATERMARK_FILE, "r") as f:
                data = json.load(f)
                return data.get(source_name, "1970-01-01 00:00:00")
    except: pass
    return "1970-01-01 00:00:00"

def set_last_ts(source_name, ts):
    try:
        data = {}
        if os.path.exists(WATERMARK_FILE):
            with open(WATERMARK_FILE, "r") as f: data = json.load(f)
        data[source_name] = str(ts)
        with open(WATERMARK_FILE, "w") as f: json.dump(data, f)
    except: pass

def reset_and_init_es():
    es_url = f"http://{ES_HOST}:{ES_PORT}/{ES_INDEX_NAME}"
    try:
        req_del = urllib.request.Request(es_url, method="DELETE")
        urllib.request.urlopen(req_del)
        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ: {ES_INDEX_NAME}")
    except: pass
    
    mapping = { "mappings": { "properties": {
        "final_ts": { "type": "date", "format": "yyyy-MM-dd+HH:mm||yyyy-MM-dd||yyyy-MM-dd'T'HH:mm:ss||yyyy-MM-dd HH:mm:ss||strict_date_optional_time||epoch_millis" },
        "log_source": { "type": "keyword" }, "user_id": { "type": "keyword" },
        "action": { "type": "keyword" }, "risk_score": { "type": "double" },
        "emp_name": { "type": "keyword" }
    }}}
    try:
        req = urllib.request.Request(es_url, data=json.dumps(mapping).encode("utf-8"), method="PUT")
        req.add_header("Content-Type", "application/json")
        urllib.request.urlopen(req)
        logger.info(f"âœ… ES ë§¤í•‘ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e: logger.error(f"âŒ ES ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# --- [í•µì‹¬ íŒŒì´í”„ë¼ì¸ ì—”ì§„] ---
def run_pipeline(spark, active_plugins):
    auto_learn_and_save_parsers()

    sources = []
    source_files = glob.glob(f"{CONFIG_DIR}/*_sources.json")
    for file_path in source_files:
        with open(file_path, "r", encoding="utf-8") as f:
            sources.extend([s for s in json.load(f) if s.get("enabled", True)])

    total_processed = 0
    for source in sources:
        start_time = datetime.now()
        source_name = source.get('name', 'Unknown')
        
        # â­ï¸ [ë³µêµ¬ë¨] ì†ŒìŠ¤ë³„ ì›Œí„°ë§ˆí¬ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
        watermark_col = source.get("watermark_col", "final_ts")
        watermark_default = source.get("watermark_default", "1970-01-01 00:00:00")
        source_type = source.get("type", "").lower()
        
        try:
            last_ts = get_last_ts(source_name)
            
            # â­ï¸ [ë³µêµ¬ë¨] íŒŒì¼ì— ì €ì¥ëœ ì´ë ¥ì´ ì—†ê³  íŠ¹ë³„í•œ ê¸°ë³¸ê°’(ì˜ˆ: "0")ì´ ìˆë‹¤ë©´ êµì²´
            if last_ts == "1970-01-01 00:00:00" and watermark_default != "1970-01-01 00:00:00":
                last_ts = watermark_default
                
            # 1. ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (DB/íŒŒì¼ ì†ŒìŠ¤ ê³µí†µ)
            raw_pandas_df = fetch_data(source, last_updated=last_ts)
            
            # [ê¶ê·¹ì˜ ë°©ì–´ì„ ] Noneì´ê±°ë‚˜, ìœ ë ¹ ì»¬ëŸ¼ë§Œ ìˆëŠ” ê²½ìš° ì™„ë²½ ì°¨ë‹¨
            if raw_pandas_df is None or raw_pandas_df.empty or len(raw_pandas_df.index) == 0:
                logger.info(f"â© [{source_name}] ì‹ ê·œ ìˆ˜ì§‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (0ê±´ ê¸°ë¡)")
                save_history(source_name, 0, "SUCCESS", start_time=start_time)
                continue

            raw_pandas_df = raw_pandas_df.dropna(axis=1, how='all')
            if raw_pandas_df.empty:
                logger.info(f"â© [{source_name}] ìœ íš¨í•œ ê°’(ì•Œë§¹ì´)ì´ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (0ê±´ ê¸°ë¡)")
                save_history(source_name, 0, "SUCCESS", start_time=start_time)
                continue
                
            # â­ï¸ [í•µì‹¬ ë³µêµ¬] Sparkë¡œ ë³€í™˜í•˜ê¸° ì „, ì›ë³¸ DB ë°ì´í„°ì—ì„œ ê¸°ì¤€ ì»¬ëŸ¼(emp_id ë“±)ì˜ ìµœëŒ“ê°’ì„ ë¯¸ë¦¬ ì¶”ì¶œ!
            new_max_ts = None
            if watermark_col in raw_pandas_df.columns:
                new_max_ts = str(raw_pandas_df[watermark_col].max())

            # 2. Pandas DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            safe_pandas_df = raw_pandas_df.replace({pd.NA: None}).where(pd.notnull(raw_pandas_df), None)
            dict_list = safe_pandas_df.to_dict(orient='records')
            
            if not dict_list or len(dict_list) == 0:
                logger.info(f"â© [{source_name}] Sparkì— ë„˜ê¸¸ ìœ íš¨ ë°ì´í„° í–‰ì´ ì—†ìŠµë‹ˆë‹¤. (0ê±´ ê¸°ë¡)")
                save_history(source_name, 0, "SUCCESS", start_time=start_time)
                continue

            # 3. ì•ˆì „ì´ í™•ë³´ëœ ë°ì´í„°ë§Œ Spark DataFrameìœ¼ë¡œ ìƒì„±
            spark_df = spark.createDataFrame(dict_list)
            clean_df = normalize_data(spark, spark_df, source_name)
            
            # â­ï¸ [ë³µêµ¬ë¨] DB ì†ŒìŠ¤ëŠ” ì¿¼ë¦¬ì—ì„œ í•„í„°ë§í–ˆìœ¼ë¯€ë¡œ, íŒŒì¼ ì†ŒìŠ¤ì¼ ë•Œë§Œ Spark ë©”ëª¨ë¦¬ 2ì°¨ í•„í„°ë§
            if source_type == "file":
                clean_df = clean_df.filter(col("final_ts").cast("string") > lit(str(last_ts)))
                
            current_count = clean_df.count()
            
            if current_count == 0:
                logger.info(f"â© [{source_name}] ì›Œí„°ë§ˆí¬ ì´í›„ ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (0ê±´ ê¸°ë¡)")
                save_history(source_name, 0, "SUCCESS", start_time=start_time)
                continue
                
            # 4. ìˆ˜ì§‘ ì„±ê³µ ì‹œ Watermark ê°±ì‹  (ì›ë³¸ DBì—ì„œ êµ¬í•œ emp_id ë“±ì˜ ìµœëŒ“ê°’ì„ ìµœìš°ì„  ì‚¬ìš©)
            if new_max_ts:
                set_last_ts(source_name, new_max_ts)
                logger.info(f"ğŸ“ [{source_name}] Watermark ê°±ì‹  ì™„ë£Œ ({watermark_col}): {new_max_ts}")
            else:
                max_ts = clean_df.agg({"final_ts": "max"}).collect()[0][0]
                if max_ts: 
                    set_last_ts(source_name, str(max_ts))
                    logger.info(f"ğŸ“ [{source_name}] Watermark ê°±ì‹  ì™„ë£Œ: {max_ts}")

            # 5. í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰ ë° ì ì¬
            detected_df = load_and_run_plugins(clean_df, active_plugins.get("detection", []), "Detection")
            load_and_run_plugins(detected_df, active_plugins.get("loading", []), "Loading")
            
            save_history(source_name, current_count, "SUCCESS", start_time=start_time)
            total_processed += current_count

        except Exception as e:
            logger.error(f"âŒ [{source_name}] íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            save_history(source_name, 0, "FAIL", error=str(e), start_time=start_time)

    return total_processed

def load_and_run_plugins(df, plugin_list, step_name):
    for plugin_path in plugin_list:
        try:
            plugin_module = importlib.import_module(plugin_path)
            if hasattr(plugin_module, "execute"): df = plugin_module.execute(df)
        except Exception as e: logger.error(f"âŒ {step_name} í”ŒëŸ¬ê·¸ì¸ {plugin_path} ì‹¤íŒ¨: {e}")
    return df

def main():
    logger.info("ğŸš€ UEBA ììœ¨ ì£¼í–‰ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™")
    reset_and_init_es()
    
    spark = get_spark_session()
    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
    
    with open("/UEBA/pipeline_config.json", "r") as f:
        config = json.load(f)
    active_plugins = config.get("active_plugins", {})

    try:
        while True:
            logger.info(f"\n--- {datetime.now()} ìˆ˜ì§‘ ì£¼ê¸° ì‹œì‘ ---")
            count = run_pipeline(spark, active_plugins)
            time.sleep(30)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ìˆ˜ì§‘ ì—”ì§„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()