import sys
if "/UEBA" not in sys.path:
    sys.path.insert(0, "/UEBA")
import os
import json
import importlib
import urllib.request
import glob
from common.setup.spark_manager import get_spark_session
from common.setup.logger import get_logger
from common.setup.config import ES_HOST, ES_PORT, ES_INDEX_NAME, CONFIG_DIR
from common.ingestion.data_reader import fetch_data
from common.processing.normalizer import normalize_data

logger = get_logger("Orchestrator")

def reset_and_init_es():
    """Elasticsearch ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë§¤í•‘ ì„¤ì •"""
    es_url = f"http://{ES_HOST}:{ES_PORT}/{ES_INDEX_NAME}"
    try:
        req_del = urllib.request.Request(es_url, method='DELETE')
        urllib.request.urlopen(req_del)
        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ: {ES_INDEX_NAME}")
    except: pass
    
    # [ìˆ˜ì •] final_ts í•„ë“œë¥¼ ëª…ì‹œì ìœ¼ë¡œ date íƒ€ì…ìœ¼ë¡œ ì„¤ì • (ë°©í™”ë²½ ë¡œê·¸ í˜¸í™˜ì„±)
    mapping = { "mappings": { "properties": {
        "final_ts": { "type": "date", "format": "yyyy-MM-dd'T'HH:mm:ss||yyyy-MM-dd HH:mm:ss||strict_date_optional_time||epoch_millis" },
        "log_source": { "type": "keyword" }, "user_id": { "type": "keyword" }, "action": { "type": "keyword" },
        "src_ip": { "type": "keyword" }, "department": { "type": "keyword" }, "salary": { "type": "double" },
        "risk_score": { "type": "double" }, "alert_message": { "type": "keyword" }
    }}}
    try:
        req = urllib.request.Request(es_url, data=json.dumps(mapping).encode('utf-8'), method='PUT')
        req.add_header('Content-Type', 'application/json')
        urllib.request.urlopen(req)
        logger.info(f"âœ… ì¸ë±ìŠ¤ ë§¤í•‘ ì´ˆê¸°í™” ì„±ê³µ: {ES_INDEX_NAME}")
    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def load_and_run_plugins(df, plugin_list, step_name):
    """í”ŒëŸ¬ê·¸ì¸ ë™ì  ë¡œë“œ ë° ì‹¤í–‰"""
    for plugin_path in plugin_list:
        logger.info(f"[{step_name}] í”ŒëŸ¬ê·¸ì¸ ê°€ë™: {plugin_path}")
        try:
            plugin_module = importlib.import_module(plugin_path)
            if hasattr(plugin_module, 'execute'):
                df = plugin_module.execute(df)
            else:
                logger.error(f"í”ŒëŸ¬ê·¸ì¸ {plugin_path}ì— 'execute' í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"í”ŒëŸ¬ê·¸ì¸ {plugin_path} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    return df

def main():
    logger.info("====== [í”ŒëŸ¬ê·¸ì¸ ê¸°ë°˜] UEBA íŒŒì´í”„ë¼ì¸ ê°€ë™ ======")
    reset_and_init_es()
    spark = get_spark_session()
    
    # 1. íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì„¤ì • ì½ê¸°
    config_path = "/UEBA/pipeline_config.json"
    with open(config_path, 'r') as f:
        config = json.load(f)
    active_plugins = config.get("active_plugins", {})

    # 2. ëª¨ë“  ì„¤ì • íŒŒì¼(*_sources.json) í†µí•© ì½ê¸°
    sources = []
    source_files = glob.glob(f"{CONFIG_DIR}/*_sources.json")
    
    for file_path in source_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                file_sources = json.load(f)
                for s in file_sources:
                    if s.get("enabled", True): 
                        sources.append(s)
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path}): {e}")

    # 3. ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìˆœì°¨ ì²˜ë¦¬
    for source in sources:
        logger.info(f"\n--- ë°ì´í„° ìŠ¤íŠ¸ë¦¼: {source.get('name')} ---")
        
        # [í•µì‹¬] ë°ì´í„° ìˆ˜ì§‘
        raw_pandas_df = fetch_data(source)
        
        # [ë°©ì–´ ë¡œì§] ë°ì´í„°ê°€ ì—†ìœ¼ë©´ Spark ë³€í™˜ì„ ê±´ë„ˆëœ€ (IndexError ë°©ì§€)
        if raw_pandas_df is None or raw_pandas_df.empty:
            logger.warning(f"âš ï¸ [{source.get('name')}] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        # ì •ê·œí™” ë° í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰
        clean_df = normalize_data(spark, raw_pandas_df, source.get('name'))
        detected_df = load_and_run_plugins(clean_df, active_plugins.get("detection", []), "Detection")
        load_and_run_plugins(detected_df, active_plugins.get("loading", []), "Loading")
        load_and_run_plugins(detected_df, active_plugins.get("notification", []), "Notification")

    spark.stop()
    logger.info("====== ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ ======")

if __name__ == "__main__":
    main()