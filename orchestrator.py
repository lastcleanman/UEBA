import sys
import traceback
import os
import json
import importlib
import urllib.request
import glob
import time
from datetime import datetime
import pandas as pd
import xml.etree.ElementTree as ET
from xml.dom import minidom
from sqlalchemy import create_engine, text

if "/UEBA" not in sys.path:
    sys.path.insert(0, "/UEBA")

from common.setup.spark_manager import get_spark_session
from common.setup.logger import get_logger
from common.setup.config import ES_HOST, ES_PORT, ES_INDEX_NAME, CONFIG_DIR
from common.ingestion.data_reader import fetch_data
from common.processing.normalizer import normalize_data
from pyspark.sql.functions import col, lit

logger = get_logger("Orchestrator")

WATERMARK_FILE = "/UEBA/watermark.json"
PARSER_DIR = "/UEBA/common/parser"
DB_SOURCES_PATH = "/UEBA/common/setup/db_sources.json" # â­ï¸ DB ì„¤ì • íŒŒì¼ ê²½ë¡œ

# --- [ì¶”ê°€] DB ì„¤ì •ì„ JSONì—ì„œ ì½ì–´ Engineì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ---

def get_db_engine_by_name(db_name="ueba_mariaDB"):
    """json ì„¤ì • íŒŒì¼ì—ì„œ ì´ë¦„ìœ¼ë¡œ DB ì ‘ì† ì •ë³´ë¥¼ ì°¾ì•„ SQLAlchemy Engineì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if not os.path.exists(DB_SOURCES_PATH):
            logger.error(f"âŒ DB ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DB_SOURCES_PATH}")
            return None
            
        with open(DB_SOURCES_PATH, "r", encoding="utf-8") as f:
            # â­ï¸ í•¨ìˆ˜ ì•ˆì—ì„œ ì§ì ‘ ì½ì–´ì„œ 'sources' ë¯¸ì •ì˜ ì—ëŸ¬ ë°©ì§€
            data_sources = json.load(f)
            
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°ì™€ ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ëª¨ë‘ ëŒ€ì‘
        if isinstance(data_sources, list):
            conf = next((s for s in data_sources if s.get("name") == db_name), None)
        else:
            conf = data_sources if data_sources.get("name") == db_name else None

        if not conf:
            logger.error(f"âŒ '{db_name}' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        # â­ï¸ ì œê³µí•´ì£¼ì‹  JSONì˜ 'database' í‚¤ë¥¼ ì •í™•íˆ ì½ì–´ì˜´
        target_db = conf.get('database')
        if not target_db:
            logger.error(f"âŒ '{db_name}' ì„¤ì •ì— 'database' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        # SQLAlchemy URL ìƒì„±
        db_url = f"mysql+pymysql://{conf['user']}:{conf['password']}@{conf['host']}:{conf['port']}/{target_db}"
        return create_engine(db_url, pool_pre_ping=True)
        
    except Exception as e:
        logger.error(f"âŒ DB ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ì „ì—­ ì—”ì§„ ë³€ìˆ˜ ì´ˆê¸°í™”
db_engine = get_db_engine_by_name("ueba_mariaDB")

# --- [Step 1~3] ììœ¨ í•™ìŠµ ë° íŒŒì„œ ìƒì„±/ì €ì¥ ë¡œì§ ---

def auto_learn_and_save_parsers():
    """ë¡œê·¸ íŒ¨í„´ í•™ìŠµ í›„ DBì™€ ë¬¼ë¦¬ íŒŒì¼ì— ë™ì‹œ ì €ì¥ (DB ì—”ì§„ ë™ì  í™œìš©)"""
    if db_engine is None: return
    
    logger.info("ğŸ•µï¸ [Step 1-3] ì‹ ê·œ íŒ¨í„´ í•™ìŠµ ë° íŒŒì„œ ì—…ë°ì´íŠ¸ ì‹œì‘")
    if not os.path.exists(PARSER_DIR): os.makedirs(PARSER_DIR, exist_ok=True)

    log_files = glob.glob("/UEBA/data/logs/*.log")
    inference_map = {
        "user": "user_id", "user_id": "user_id",
        "ip": "src_ip", "src_ip": "src_ip",
        "department": "department", "action": "action",
        "device_id": "device_id", "resource": "resource"
    }

    for file_path in log_files:
        filename = os.path.basename(file_path)
        source_name = "Unknown"
        if "authentication" in filename: source_name = "Auth_Logs"
        elif "webserver" in filename: source_name = "Web_Logs"
        elif "endpoint" in filename: source_name = "Endpoint_Logs"
        elif "firewall" in filename: source_name = "Firewall_Logs"
        
        if source_name == "Unknown": continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                line = f.readline().strip()
                if not line: continue
                sample = json.loads(line)
            
            root = ET.Element("parser", name=source_name)
            has_user = False
            for k, v in sample.items():
                if k in inference_map:
                    target = inference_map[k]
                    ET.SubElement(root, "field", target=target, source=k)
                    if target == "user_id": has_user = True
            
            if has_user:
                ET.SubElement(root, "field", target="emp_name", source="mapped_name")

            xml_str = minidom.parseString(ET.tostring(root)).toprettyxml(indent="    ")

            # íŒŒì¼ ì €ì¥
            with open(os.path.join(PARSER_DIR, f"{source_name}.xml"), "w", encoding="utf-8") as xf:
                xf.write(xml_str)
            
            # DB ì €ì¥
            with db_engine.begin() as conn:
                conn.execute(text("""
                    INSERT INTO sj_ueba_parsers (source_name, parser_xml)
                    VALUES (:source, :xml)
                    ON DUPLICATE KEY UPDATE parser_xml = :xml, updated_at = CURRENT_TIMESTAMP
                """), {"source": source_name, "xml": xml_str})
            logger.info(f"âœ… [{source_name}] íŒŒì„œ ë™ê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ [{source_name}] í•™ìŠµ ì‹¤íŒ¨: {e}")

# --- [Step 4] ìˆ˜ì§‘ ì´ë ¥ ê´€ë¦¬ ë¡œì§ ---

def save_history(source, count, status, error="", start_time=None):
    if db_engine is None: return
    try:
        with db_engine.begin() as conn:
            conn.execute(text("""
                INSERT INTO sj_ueba_ingestion_history (source_name, processed_count, status, error_message, start_time)
                VALUES (:source, :count, :status, :error, :start)
            """), {
                "source": source, "count": count, "status": status, "error": error, "start": start_time
            })
        logger.info(f"ğŸ“œ [History] {source} ì²˜ë¦¬ ì´ë ¥ ê¸°ë¡ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ DB ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")

# --- ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ë¡œì§ (ìˆ˜ì • ë° ìœ ì§€) ---

def get_last_ts(source_name):
    try:
        if os.path.exists(WATERMARK_FILE):
            with open(WATERMARK_FILE, "r") as f:
                data = json.load(f)
                return data.get(source_name, "1970-01-01 00:00:00")
    except: pass
    return "1970-01-01 00:00:00"

def set_last_ts(source_name, ts):
    try:
        data = {}
        if os.path.exists(WATERMARK_FILE):
            with open(WATERMARK_FILE, "r") as f: data = json.load(f)
        data[source_name] = str(ts)
        with open(WATERMARK_FILE, "w") as f: json.dump(data, f)
    except: pass

def reset_and_init_es():
    es_url = f"http://{ES_HOST}:{ES_PORT}/{ES_INDEX_NAME}"
    try:
        req_del = urllib.request.Request(es_url, method="DELETE")
        urllib.request.urlopen(req_del)
        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ: {ES_INDEX_NAME}")
    except: pass
    
    mapping = { "mappings": { "properties": {
        "final_ts": { "type": "date", "format": "yyyy-MM-dd+HH:mm||yyyy-MM-dd||yyyy-MM-dd'T'HH:mm:ss||yyyy-MM-dd HH:mm:ss||strict_date_optional_time||epoch_millis" },
        "log_source": { "type": "keyword" }, "user_id": { "type": "keyword" },
        "action": { "type": "keyword" }, "risk_score": { "type": "double" },
        "emp_name": { "type": "keyword" }  # â­ï¸ ì´ë¦„ í•„ë“œ ì¶”ê°€
    }}}
    try:
        req = urllib.request.Request(es_url, data=json.dumps(mapping).encode("utf-8"), method="PUT")
        req.add_header("Content-Type", "application/json")
        urllib.request.urlopen(req)
        logger.info(f"âœ… ES ë§¤í•‘ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e: logger.error(f"âŒ ES ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

def run_pipeline(spark, active_plugins):
    # ë§¤ ì£¼ê¸° ì‹œì‘ ì‹œ í•™ìŠµ ë¨¼ì € ìˆ˜í–‰ (Step 1-3)
    auto_learn_and_save_parsers()

    sources = []
    source_files = glob.glob(f"{CONFIG_DIR}/*_sources.json")
    for file_path in source_files:
        with open(file_path, "r", encoding="utf-8") as f:
            sources.extend([s for s in json.load(f) if s.get("enabled", True)])

    total_processed = 0
    for source in sources:
        start_time = datetime.now()
        source_name = source.get('name', 'Unknown')
        
        try:
            # ë°ì´í„° ìˆ˜ì§‘ (DBì—ì„œ ì‹¤ì‹œê°„ ìƒì„±ëœ íŒŒì„œ ì°¸ì¡°)
            raw_pandas_df = fetch_data(source)
            if raw_pandas_df is None or raw_pandas_df.empty: continue
            
            safe_pandas_df = raw_pandas_df.replace({pd.NA: None}).where(pd.notnull(raw_pandas_df), None)
            dict_list = safe_pandas_df.to_dict(orient='records')
            spark_df = spark.createDataFrame(dict_list)

            # ì •ì œ ë° ë§µí•‘ (Step 4)
            clean_df = normalize_data(spark, spark_df, source_name)
            
            last_ts = get_last_ts(source_name)
            clean_df = clean_df.filter(col("final_ts").cast("string") > lit(str(last_ts)))
            current_count = clean_df.count()
            
            if current_count == 0:
                logger.info(f"â© [{source_name}] ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            # Watermark ê°±ì‹ 
            max_ts = clean_df.agg({"final_ts": "max"}).collect()[0][0]
            if max_ts: set_last_ts(source_name, max_ts)

            # í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰ (Elastic ì ì¬ ë“±)
            detected_df = load_and_run_plugins(clean_df, active_plugins.get("detection", []), "Detection")
            load_and_run_plugins(detected_df, active_plugins.get("loading", []), "Loading")
            
            # ì²˜ë¦¬ ì™„ë£Œ ì´ë ¥ ì €ì¥ (Step 4)
            save_history(source_name, current_count, "SUCCESS", start_time=start_time)
            total_processed += current_count

        except Exception as e:
            logger.error(f"âŒ [{source_name}] íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            save_history(source_name, 0, "FAIL", error=str(e), start_time=start_time)

    return total_processed

def load_and_run_plugins(df, plugin_list, step_name):
    for plugin_path in plugin_list:
        try:
            plugin_module = importlib.import_module(plugin_path)
            if hasattr(plugin_module, "execute"): df = plugin_module.execute(df)
        except Exception as e: logger.error(f"âŒ {step_name} í”ŒëŸ¬ê·¸ì¸ {plugin_path} ì‹¤íŒ¨: {e}")
    return df

def main():
    logger.info("ğŸš€ UEBA ììœ¨ ì£¼í–‰ ìˆ˜ì§‘ ì—”ì§„ ê°€ë™")
    reset_and_init_es()
    
    spark = get_spark_session()
    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
    
    with open("/UEBA/pipeline_config.json", "r") as f:
        config = json.load(f)
    active_plugins = config.get("active_plugins", {})

    try:
        while True:
            logger.info(f"\n--- {datetime.now()} ìˆ˜ì§‘ ì£¼ê¸° ì‹œì‘ ---")
            count = run_pipeline(spark, active_plugins)
            time.sleep(30)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ìˆ˜ì§‘ ì—”ì§„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()