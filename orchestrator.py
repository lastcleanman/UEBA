import sys
import traceback  # [ì¶”ê°€] ìƒì„¸ ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ
if "/UEBA" not in sys.path:
    sys.path.insert(0, "/UEBA")
import os
import json
import importlib
import urllib.request
import glob
from common.setup.spark_manager import get_spark_session
from common.setup.logger import get_logger
from common.setup.config import ES_HOST, ES_PORT, ES_INDEX_NAME, CONFIG_DIR
from common.ingestion.data_reader import fetch_data
from common.processing.normalizer import normalize_data

logger = get_logger("Orchestrator")

def reset_and_init_es():
    """Elasticsearch ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë§¤í•‘ ì„¤ì •"""
    # URL ìƒì„± ë¡œê·¸ ì¶”ê°€
    es_url = f"http://{ES_HOST}:{ES_PORT}/{ES_INDEX_NAME}"
    logger.info(f"ğŸ” Elasticsearch ì ‘ì† ì‹œë„: {es_url}")

    try:
        req_del = urllib.request.Request(es_url, method="DELETE")
        urllib.request.urlopen(req_del)
        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ: {ES_INDEX_NAME}")
    except Exception as e:
        # ì‚­ì œ ì‹¤íŒ¨ëŠ” ì¸ë±ìŠ¤ê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ê²½ê³ ë§Œ í•˜ê³  ë„˜ì–´ê° (ìƒì„¸ ë¡œê·¸ëŠ” ìƒëµ ê°€ëŠ¥)
        logger.warning(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ê±´ë„ˆëœ€ (ì—†ê±°ë‚˜ ì ‘ì† ì‹¤íŒ¨): {e}")
    
    mapping = { "mappings": { "properties": {
        "final_ts": { "type": "date", "format": "yyyy-MM-dd'T'HH:mm:ss||yyyy-MM-dd HH:mm:ss||strict_date_optional_time||epoch_millis" },
        "log_source": { "type": "keyword" }, "user_id": { "type": "keyword" }, "action": { "type": "keyword" },
        "src_ip": { "type": "keyword" }, "department": { "type": "keyword" }, "salary": { "type": "double" },
        "risk_score": { "type": "double" }, "alert_message": { "type": "keyword" }
    }}}
    
    try:
        req = urllib.request.Request(es_url, data=json.dumps(mapping).encode("utf-8"), method="PUT")
        req.add_header("Content-Type", "application/json")
        urllib.request.urlopen(req)
        logger.info(f"âœ… ì¸ë±ìŠ¤ ë§¤í•‘ ì´ˆê¸°í™” ì„±ê³µ: {ES_INDEX_NAME}")
    except Exception as e:
        logger.error("âŒ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ!")
        logger.error(f"ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        logger.error("ğŸ‘‡ ì•„ë˜ ìƒì„¸ ë¡œê·¸(Traceback)ë¥¼ í™•ì¸í•˜ì„¸ìš” ğŸ‘‡")
        traceback.print_exc()  # [í•µì‹¬] ì—¬ê¸°ì„œ ì—ëŸ¬ì˜ ë¿Œë¦¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

def load_and_run_plugins(df, plugin_list, step_name):
    """í”ŒëŸ¬ê·¸ì¸ ë™ì  ë¡œë“œ ë° ì‹¤í–‰"""
    for plugin_path in plugin_list:
        logger.info(f"[{step_name}] í”ŒëŸ¬ê·¸ì¸ ê°€ë™: {plugin_path}")
        try:
            plugin_module = importlib.import_module(plugin_path)
            if hasattr(plugin_module, "execute"):
                df = plugin_module.execute(df)
            else:
                logger.error(f"í”ŒëŸ¬ê·¸ì¸ {plugin_path}ì— 'execute' í•¨ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"í”ŒëŸ¬ê·¸ì¸ {plugin_path} ì‹¤í–‰ ì‹¤íŒ¨")
            traceback.print_exc() # í”ŒëŸ¬ê·¸ì¸ ì—ëŸ¬ë„ ìƒì„¸íˆ ì¶œë ¥
    return df

def main():
    logger.info("====== [í”ŒëŸ¬ê·¸ì¸ ê¸°ë°˜] UEBA íŒŒì´í”„ë¼ì¸ ê°€ë™ ======")
    
    # ì„¤ì • íŒŒì¼ ë‚´ìš©ì„ ë¨¼ì € ì°ì–´ë´…ë‹ˆë‹¤ (ë””ë²„ê¹…ìš©)
    logger.info(f"ğŸ”§ í˜„ì¬ ì„¤ì •ëœ ES ì •ë³´: Host={ES_HOST}, Port={ES_PORT}")

    reset_and_init_es()
    spark = get_spark_session()
    
    # 1. íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì„¤ì • ì½ê¸°
    config_path = "/UEBA/pipeline_config.json"
    with open(config_path, "r") as f:
        config = json.load(f)
    active_plugins = config.get("active_plugins", {})

    # 2. ëª¨ë“  ì„¤ì • íŒŒì¼(*_sources.json) í†µí•© ì½ê¸°
    sources = []
    source_files = glob.glob(f"{CONFIG_DIR}/*_sources.json")
    
    for file_path in source_files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                file_sources = json.load(f)
                for s in file_sources:
                    if s.get("enabled", True): 
                        sources.append(s)
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path}): {e}")

    # 3. ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìˆœì°¨ ì²˜ë¦¬
    for source in sources:
        logger.info(f"\n--- ë°ì´í„° ìŠ¤íŠ¸ë¦¼: {source.get('name')} ---")
        
        raw_pandas_df = fetch_data(source)
        
        if raw_pandas_df is None or raw_pandas_df.empty:
            logger.warning(f"âš ï¸ [{source.get('name')}] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        clean_df = normalize_data(spark, raw_pandas_df, source.get('name'))
        detected_df = load_and_run_plugins(clean_df, active_plugins.get("detection", []), "Detection")
        load_and_run_plugins(detected_df, active_plugins.get("loading", []), "Loading")
        load_and_run_plugins(detected_df, active_plugins.get("notification", []), "Notification")

    spark.stop()
    logger.info("====== ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ ======")

if __name__ == "__main__":
    main()
