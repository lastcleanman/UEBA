from common.setup.logger import get_logger
from pyspark.sql.functions import col, lit, md5, concat_ws, coalesce, lower, trim

logger = get_logger("Plugin-RDBLoad")

def execute(df):
    logger.info("π’Ύ [RDB μ μ¬] μ „μ²΄ λ΅μ° μ§€λ¬Έ λ€μ΅° λ¨λ“ κ°€λ™ (λ³€λ™ μ»¬λΌ μ μ™Έ)...")

    jdbc_url = "jdbc:mysql://192.168.0.131:13306/UEBA_TEST?useSSL=false"
    db_props = {"user": "ueba_user", "password": "Suju!0901", "driver": "com.mysql.cj.jdbc.Driver"}
    table_name = "ueba_alerts"

    try:
        # 1. DB μ¤ν‚¤λ§ λ™μ  λ¶„μ„
        db_schema = df.sparkSession.read.jdbc(url=jdbc_url, table=table_name, properties=db_props).schema
        actual_cols = df.columns
        
        # 2. μ μ¬μ© μ»¬λΌ κµ¬μ„± λ° μ§€λ¬Έ(row_id) μƒμ„±μ© μ»¬λΌ μ„ λ³„
        select_exprs = []
        # μ§€λ¬Έ μƒμ„±μ—μ„ μ μ™Έν•  'λ§¤λ² λ³€ν•λ” μ»¬λΌ' λ¦¬μ¤νΈ
        # μ‹κ°„μ„ λΉΌμ§€ λ§λΌκ³  ν•μ…¨μΌλ‚, μ¤‘λ³µ μ²΄ν¬μ 'κΈ°μ¤€'μ—μ„λ” μ μ™Έν•΄μ•Ό λ™μΌ λ°μ΄ν„°λ΅ μΈμ‹λ©λ‹λ‹¤.
        # (μ‹¤μ  λ°μ΄ν„° κ°’μΌλ΅λ” λ“¤μ–΄κ°‘λ‹λ‹¤.)
        exclude_from_hash = ["row_id", "created_at", "final_ts", "timestamp"] 
        hash_targets = []

        for field in db_schema:
            f_name = field.name
            if f_name in ["row_id", "created_at"]: continue

            if f_name in actual_cols:
                select_exprs.append(col(f_name).cast(field.dataType))
            elif f_name == "emp_id" and "employee_id" in actual_cols:
                select_exprs.append(col("employee_id").cast(field.dataType).alias("emp_id"))
            else:
                select_exprs.append(lit(None).cast(field.dataType).alias(f_name))
            
            # μ§€λ¬Έ μƒμ„±μ© λ¦¬μ¤νΈ (λ³€λ™ μ»¬λΌ μ μ™Έν• λ‚λ¨Έμ§€ μ „μ²΄)
            if f_name not in exclude_from_hash:
                hash_targets.append(lower(trim(coalesce(col(f_name).cast("string"), lit("null")))))

        # 3. λ°μ΄ν„°ν”„λ μ„ κµ¬μ„± λ° row_id μƒμ„±
        # μ‹¤μ  λ°μ΄ν„°λ” λ¨λ“  κ°’μ„ μ μ§€ν•λ, row_id(μ§€λ¬Έ)λ” λ³€ν•μ§€ μ•λ” κ°’λ“¤λ΅λ§ λ§λ“­λ‹λ‹¤.
        df_base = df.select(*select_exprs)
        df_with_id = df_base.withColumn("row_id", md5(concat_ws("|", *hash_targets)))

        # 4. DBμ™€ λ€μ΅°ν•μ—¬ μ¤‘λ³µ μ κ±°
        df_existing = df.sparkSession.read.jdbc(url=jdbc_url, table=f"(SELECT row_id FROM {table_name}) as t", properties=db_props)
        df_final = df_with_id.join(df_existing, "row_id", "left_anti").dropDuplicates(["row_id"])
        
        new_count = df_final.count()
        if new_count > 0:
            logger.info(f"β¨ μ§€λ¬Έ λ€μ΅° μ™„λ£: μƒλ΅μ΄ λ°μ΄ν„° {new_count}κ±΄μ„ μ μ¬ν•©λ‹λ‹¤.")
            df_final.write.mode("append").jdbc(url=jdbc_url, table=table_name, properties=db_props)
            logger.info("β… [RDB μ μ¬] μ™„λ£.")
        else:
            logger.info("π λ¨λ“  λ°μ΄ν„°κ°€ λ‚΄μ©μƒ μ΄λ―Έ DBμ— μ΅΄μ¬ν•©λ‹λ‹¤. (μ μ¬ μ¤ν‚µ)")

    except Exception as e:
        logger.error(f"β μ μ¬ ν”„λ΅μ„Έμ¤ μ¤‘ μ¤λ¥ λ°μƒ: {e}")
        
    return df